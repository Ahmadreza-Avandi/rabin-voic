// Speech Recognition Interface
interface SpeechRecognitionOptions {
  onResult: (transcript: string) => void;
  onEnd: (finalTranscript: string) => void;
  onError: (error: string) => void;
}

// Check if speech recognition is supported
export const isSpeechRecognitionSupported = (): boolean => {
  return 'webkitSpeechRecognition' in window || 'SpeechRecognition' in window;
};

// Start listening to user speech
export const startSpeechRecognition = (options: SpeechRecognitionOptions): any => {
  if (!isSpeechRecognitionSupported()) {
    options.onError('ØªØ´Ø®ÛŒØµ Ú¯ÙØªØ§Ø± Ø¯Ø± Ø§ÛŒÙ† Ù…Ø±ÙˆØ±Ú¯Ø± Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯');
    return null;
  }

  console.log('ğŸ¤ Ø´Ø±ÙˆØ¹ ØªØ´Ø®ÛŒØµ Ú¯ÙØªØ§Ø±...');

  const SpeechRecognition = (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition;
  const recognition = new SpeechRecognition();

  recognition.lang = 'fa-IR';
  recognition.continuous = false;
  recognition.interimResults = true;
  recognition.maxAlternatives = 1;
  
  // Add timeout settings
  recognition.grammars = null;
  recognition.serviceURI = '';

  let finalTranscript = '';
  let interimTranscript = '';

  recognition.onresult = (event: any) => {
    console.log('ğŸ¯ Ù†ØªÛŒØ¬Ù‡ ØªØ´Ø®ÛŒØµ Ú¯ÙØªØ§Ø± Ø¯Ø±ÛŒØ§ÙØª Ø´Ø¯');
    interimTranscript = '';
    
    for (let i = event.resultIndex; i < event.results.length; i++) {
      const transcript = event.results[i][0].transcript;
      
      if (event.results[i].isFinal) {
        finalTranscript += transcript;
        console.log('âœ… Ù…ØªÙ† Ù†Ù‡Ø§ÛŒÛŒ:', finalTranscript);
      } else {
        interimTranscript += transcript;
        console.log('â³ Ù…ØªÙ† Ù…ÙˆÙ‚Øª:', interimTranscript);
      }
    }

    // Send current transcript (final + interim)
    const currentTranscript = finalTranscript + interimTranscript;
    options.onResult(currentTranscript);
  };

  recognition.onend = () => {
    console.log('ğŸ”š ØªØ´Ø®ÛŒØµ Ú¯ÙØªØ§Ø± Ù¾Ø§ÛŒØ§Ù† ÛŒØ§ÙØª');
    // Clean up transcript and send final result
    const cleanTranscript = finalTranscript.trim();
    options.onEnd(cleanTranscript);
  };

  recognition.onstart = () => {
    console.log('â–¶ï¸ ØªØ´Ø®ÛŒØµ Ú¯ÙØªØ§Ø± Ø´Ø±ÙˆØ¹ Ø´Ø¯');
  };

  recognition.onerror = (event: any) => {
    console.error('âŒ Ø®Ø·Ø§ Ø¯Ø± ØªØ´Ø®ÛŒØµ Ú¯ÙØªØ§Ø±:', event.error);
    let errorMessage = 'Ø®Ø·Ø§ÛŒ Ù†Ø§Ù…Ø´Ø®Øµ Ø¯Ø± ØªØ´Ø®ÛŒØµ Ú¯ÙØªØ§Ø±';
    
    switch (event.error) {
      case 'no-speech':
        errorMessage = 'ØµØ¯Ø§ÛŒÛŒ Ø´Ù†ÛŒØ¯Ù‡ Ù†Ø´Ø¯. Ø¯Ø± Ø­Ø§Ù„ ØªÙ„Ø§Ø´ Ù…Ø¬Ø¯Ø¯...';
        break;
      case 'audio-capture':
        errorMessage = 'Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ Ù…ÛŒÚ©Ø±ÙˆÙÙˆÙ†';
        break;
      case 'not-allowed':
        errorMessage = 'Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ Ù…ÛŒÚ©Ø±ÙˆÙÙˆÙ† Ø±Ø¯ Ø´Ø¯';
        break;
      case 'network':
        errorMessage = 'Ø®Ø·Ø§ Ø¯Ø± Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø§ÛŒÙ†ØªØ±Ù†Øª';
        break;
      case 'service-not-allowed':
        errorMessage = 'Ø³Ø±ÙˆÛŒØ³ ØªØ´Ø®ÛŒØµ Ú¯ÙØªØ§Ø± Ø¯Ø±Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª';
        break;
      case 'aborted':
        errorMessage = 'ØªØ´Ø®ÛŒØµ Ú¯ÙØªØ§Ø± Ù„ØºÙˆ Ø´Ø¯';
        break;
    }
    
    options.onError(errorMessage);
  };

  try {
    recognition.start();
  } catch (error) {
    console.error('âŒ Ø®Ø·Ø§ Ø¯Ø± Ø´Ø±ÙˆØ¹ ØªØ´Ø®ÛŒØµ Ú¯ÙØªØ§Ø±:', error);
    options.onError('Ø®Ø·Ø§ Ø¯Ø± Ø´Ø±ÙˆØ¹ ØªØ´Ø®ÛŒØµ Ú¯ÙØªØ§Ø±');
  }
  
  return recognition;
};

// Stop listening
export const stopListening = (recognition: any): void => {
  if (recognition) {
    recognition.stop();
  }
};

// Convert text to speech using external API
export const textToSpeech = async (text: string): Promise<string> => {
  try {
    const API_BASE_URL = process.env.NODE_ENV === 'production' 
      ? '/api' 
      : 'http://localhost:3001/api';
      
    const response = await fetch(`${API_BASE_URL}/tts/convert`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({ text }),
    });

    if (!response.ok) {
      throw new Error('Ø®Ø·Ø§ Ø¯Ø± ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† Ø¨Ù‡ Ú¯ÙØªØ§Ø±');
    }

    const data = await response.json();
    
    if (!data.success) {
      throw new Error(data.error || 'Ø®Ø·Ø§ Ø¯Ø± ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† Ø¨Ù‡ Ú¯ÙØªØ§Ø±');
    }

    console.log('TTS Response:', data);
    
    // Return audio URL
    return data.audioUrl;
  } catch (error) {
    console.error('TTS Error:', error);
    throw new Error('Ø®Ø·Ø§ Ø¯Ø± ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† Ø¨Ù‡ Ú¯ÙØªØ§Ø±');
  }
};

// Play audio from URL or base64
export const playAudio = async (text: string): Promise<void> => {
  try {
    console.log('Starting TTS for text:', text);
    
    // Get audio from TTS service
    const audioData = await textToSpeech(text);
    
    console.log('Received audio data:', audioData ? 'Success' : 'Failed');
    
    if (!audioData) {
      throw new Error('Ø¯Ø±ÛŒØ§ÙØª ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯');
    }
    
    const audioSrc = audioData;

    console.log('Audio source prepared:', audioSrc.substring(0, 50) + '...');

    // Create and play audio
    const audio = new Audio(audioSrc);
    
    // Set volume
    audio.volume = 0.8;
    
    return new Promise((resolve, reject) => {
      audio.addEventListener('loadstart', () => {
        console.log('Audio loading started');
      });
      
      audio.addEventListener('canplay', () => {
        console.log('Audio can play');
      });
      
      audio.addEventListener('loadeddata', () => {
        console.log('Audio data loaded');
      });
      
      audio.addEventListener('ended', () => resolve());
      audio.addEventListener('error', (e) => {
        console.error('Audio playback error:', e);
        console.error('Audio error details:', audio.error);
        reject(new Error(`Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø®Ø´ ØµØ¯Ø§: ${audio.error?.message || 'Ù†Ø§Ù…Ø´Ø®Øµ'}`));
      });
      
      console.log('Starting audio playback...');
      audio.play().catch((error) => {
        console.error('Audio play error:', error);
        reject(new Error(`Ø®Ø·Ø§ Ø¯Ø± Ø´Ø±ÙˆØ¹ Ù¾Ø®Ø´: ${error.message}`));
      });
    });
    
  } catch (error) {
    console.error('Play audio error:', error);
    throw error;
  }
};

// Check microphone permission
export const checkMicrophonePermission = async (): Promise<boolean> => {
  try {
    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
    stream.getTracks().forEach(track => track.stop());
    return true;
  } catch (error) {
    console.error('Microphone permission error:', error);
    return false;
  }
};